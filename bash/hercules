#!/bin/bash
#######################################################
#######################################################
StopServers() {
    NAME=$1
    hosts=$2
    echo "# Hercules: Stopping $NAME servers in $hosts"
    for node in "${hosts[@]}"
    do
        # (ssh $node "pkill hercules_server" ) &
        ssh $node "pkill hercules_server"
    done
}


HelloWorld() {
    echo "Hello in $hostname" 
}

STATUS=$1
if [[ $STATUS != "stop" && $STATUS != "start" ]]; then
    echo "Incorrect argument"
    exit 0
fi

## Verify if slurm is installed.
SLURM=$([ -z "$SLURM_CLUSTER_NAME" ] && echo 0 || echo 1)
echo "SLURM=$SLURM"
if [[ $STATUS = "stop" ]];
then
    if [[ $SLURM -eq 0 ]]; then
        ## Stops data and metadata servers.
        readarray -t hosts < data_hostfile
        echo "hosts=$hosts"
        StopServers "data" $hosts
        
        readarray -t hosts < meta_hostfile
        echo "hosts=$hosts"
        StopServers "metadata" $hosts
    fi
    exit 0
fi

## To know if a configuration file exists in the default paths.
for FILE in {"/etc/hercules.conf","../conf/hercules.conf","./hercules.conf"}
do
    echo $FILE
    if [ -f "$FILE" ]; then
        echo "Reading configuration file at $FILE"
        break
    fi
done

## Check if user pass arguments.
while getopts :m:d:o:c: flag
do
    case "${flag}" in
        m) META_SERVER_FILE=${OPTARG};;
        d) DATA_SERVER_FILE=${OPTARG};;
        o) OUTPUT_FILE=${OPTARG};;
        c) CLIENT_FILE=${OPTARG};;
    esac
done

echo "DATA_SERVER_FILE=$DATA_SERVER_FILE"
echo "META_SERVER_FILE=$META_SERVER_FILE"
echo "OUTPUT_FILE=$OUTPUT_FILE"
echo "STATUS=$1"
## If slurm is used, then we create a hostfile containing the allocated nodes.
if [[ $SLURM -eq 1 ]]; then
    srun -pernode hostname |sort > hostfile
fi

## Define values.
H_PATH=$(dirname `pwd`)
H_BUILD_PATH=$H_PATH/build
H_BASH_PATH=$H_PATH/bash
H_MPI_HOSTFILE_NAME="client_hostfile"
H_POSIX_PRELOAD="LD_PRELOAD=$H_PATH/build/tools/libhercules_posix.so"

## Read configuration file.
META_PORT=$(cat $FILE | grep "METADATA_PORT" | awk '{print $3}')
DATA_PORT=$(cat $FILE | grep "DATA_PORT" | awk '{print $3}')
MALLEABILITY=$(cat $FILE | grep "MALLEABILITY" | awk '{print $3}')
NUM_METADATA=$(cat $FILE | grep "NUM_META_SERVERS" | awk '{print $3}')
NUM_DATA=$(cat $FILE | grep "NUM_DATA_SERVERS" | awk '{print $3}')
NUM_NODES_FOR_CLIENTS=$(cat $FILE | grep "NUM_NODES_FOR_CLIENTS" | awk '{print $3}')
NUM_CLIENTS_PER_NODE=$(cat $FILE | grep "NUM_CLIENTS_PER_NODE" | awk '{print $3}')
BLOCK_SIZE=$(cat $FILE | grep "BLOCK_SIZE" | awk '{print $3}')
STORAGE_SIZE=$(cat $FILE | grep "STORAGE_SIZE" | awk '{print $3}')

## To run the metadata servers.
echo "# Hercules: Running metadata servers"
## Checks if a metadata hostfile was not defined.
if [[ -z $META_SERVER_FILE ]];
then
    ## Error if slurm is not being used and no metadata hostfile was defined.
    if [[ $SLURM -eq 0 ]]; then
        echo "Metadata server file not specified, please set one using -m <filename> flag"
        exit 0
    fi
    echo "Metadata server file not specified, getting information from slurm"
    ## If a metadata file was not defined and we have slurm's nodes allocated,
    ## then we create an array which contains the hostnames of the nodes that
    ## will be used to deploy the determinate set of metadata servers.
    readarray -t meta_hosts < <(head -n $NUM_METADATA hostfile)
else
    ## If a metadata file was defined we read it to create an array which 
    ## contains the hostnames of the nodes that will be used to deploy the 
    ## determinate set of metadata servers.
    echo "Reading metadata server file."
    readarray -t meta_hosts < $META_SERVER_FILE
fi
## To create the default metadata hostfile.
echo $meta_hosts > meta_hostfile

## The array is read to deploy the services.
i=0
for node in "${meta_hosts[@]}"
do
    echo "Running metadata server $i in $node..."
    ## If slurm is not being used, we deploy the service by connecting
    ## to the node via ssh.
    if [[ $SLURM -eq 0 ]]; then
        (ssh $node "cd $H_BASH_PATH && $H_BUILD_PATH/hercules_server m $i") &
    else
    ## If slurm is being used, the service is deploy using srun.
        srun --exclusive --export=ALL -N 1 -n 1 -w $node $H_BUILD_PATH/hercules_server m $i 2> m_server.out &
    fi
    i=$(($i+1))
done

## To wait until all the metadata servers are up.
sleep 10
# i=0
# for node in "${meta_hosts[@]}"
# do
#     echo "Running comprobation $i in $node..."
#     # (ssh $node "until [ -f /tmp/m-hercules-$1 ]; do echo "Waiting for metadata server in $node"; sleep 1; done") &
#     # srun --exclusive --export=ALL -N 1 -n 1 -w $node HelloWorld &
#     srun --exclusive --export=ALL -N 1 -n 1 -w $node until [ -f /tmp/m-hercules-$i ]; do echo "Waiting"; sleep 1; done &
#     i=$(($i+1))
# done


## To run the data servers.
echo "# Hercules: Running data servers"
if [[ -z $DATA_SERVER_FILE ]];
then
    if [[ $SLURM -eq 0 ]]; then
        echo "Data server file not specified, please set one using -d <filename> flag"
        exit 0
    fi
    echo "Data server file not specified, getting information from slurm"
    readarray -t data_hosts < <(tail -n +$((NUM_METADATA+1)) hostfile | head -n $NUM_DATA)
else
    readarray -t data_hosts < $DATA_SERVER_FILE
fi

echo $data_hosts > data_hostfile
META_NODE=${meta_hosts[0]}
NUM_METADATA=${#meta_hosts[@]}
NUM_DATA=${#data_hosts[@]}
i=0
for node in "${data_hosts[@]}"
do
    echo "Running data server $i in $node..."
    if [[ $SLURM -eq 0 ]]; then
        (ssh $node "cd $H_BASH_PATH && $H_BUILD_PATH/hercules_server d $i ${meta_hosts[0]}") &
    else
        srun --exclusive --export=ALL -N 1 -n 1 -w $node $H_BUILD_PATH/hercules_server d $i ${meta_hosts[0]} 2> d_server.out &
    fi
    i=$(($i+1))
done

#set -x
# readarray -t client_hosts < <(tail -n +$((NUM_METADATA+NUM_DATA+1)) hostfile | head -n $NUM_NODES_FOR_CLIENTS)
# echo $client_hosts
# rm client_hostfile
# for node in "${client_hosts[@]}"
# do
#     echo "Cheking $node information..."
#     # srun -N 1 -n 1 -w $node echo "Threads/core: $(nproc --all)" | awk '{print $2}' &
#     srun -N 1 -n $NUM_CLIENTS_PER_NODE -w $node hostname >> client_hostfile
# done

# tail -n +$((NUM_METADATA+NUM_DATA+1)) hostfile | head -n $NUM_NODES_FOR_CLIENTS | sed "s/$/\tslots=12/" > client_hostfile
tail -n +$((NUM_METADATA+NUM_DATA+1)) hostfile | head -n $NUM_NODES_FOR_CLIENTS > $H_MPI_HOSTFILE_NAME
export H_NCPN=$NUM_CLIENTS_PER_NODE

sleep 10

## Search for the mpi distribution installed.
for MPI_DS in {"openmpi","mpich","impi"}
do
    RET=$(which mpiexec | grep -c $MPI_DS)
    if [ $RET -gt 0 ]; then
        echo "MPI distribution found; $MPI_DS"
        break;
    fi
done

case $MPI_DS in
    "openmpi")
        echo "Option openmpi"
        export H_MPI_ENV_DEF="-x"
        export H_MPI_HOSTFILE_DEF="-hostfile"
    ;;
    "mpich" | "impi")
        echo "Option mpich | impi"
        export H_MPI_ENV_DEF="-env"
        export H_MPI_HOSTFILE_DEF="-f"
    ;;
    *)
        # Check!
        echo "No option"
        exit 0
    ;;
esac


unset META_PORT
unset DATA_PORT
unset MALLEABILITY
unset NUM_METADATA
unset NUM_DATA
unset NUM_NODES_FOR_CLIENTS
unset NUM_CLIENTS_PER_NODE
unset BLOCK_SIZE
unset STORAGE_SIZE
