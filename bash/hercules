#!/bin/bash
#######################################################
#######################################################
## To stop all servers.
StopServers() {
    NAME=$1
    hosts=$2
    echo "# Hercules: Stopping $NAME servers in $hosts"
    for node in "${hosts[@]}"
    do
        ssh $node "pkill hercules_server"
    done
}

## To wait until all the servers are up.
WaitForServers() {
    i=0
    SERVER_NAME=$1
    shift
    SERVER_TYPE=$1
    shift
    hosts=("$@")
    echo "hostsnames=${hosts[@]}"
    for node in "${hosts[@]}"
    do
        echo "[+] Running comprobation $i in $node..."
        srun --exclusive --export=ALL -N 1 -n 1 -w $node check-servers.sh $SERVER_TYPE $i
        ret=$?
        if [ "$ret" -gt 0 ]; then
            echo "[Error: $ret] It has not been possible to run a $SERVER_NAME server on $node, please verify the configuration file and logs."
            exit 1
        fi
        echo "[OK] $SERVER_NAME server running in $node"
        i=$(($i+1))
    done
}

STATUS=$1
if [[ $STATUS != "stop" && $STATUS != "start" ]]; then
    echo "Incorrect argument"
    exit 1
fi

## Verify if slurm is installed.
SLURM=$([ -z "$SLURM_CLUSTER_NAME" ] && echo 0 || echo 1)
echo "SLURM=$SLURM"
## Checks if the user wants to stop the services.
if [[ $STATUS = "stop" ]];
then
    if [[ $SLURM -eq 0 ]]; then
        ## Stops data and metadata servers.
        readarray -t hosts < data_hostfile
        echo "hosts=$hosts"
        StopServers "data" $hosts
        
        readarray -t hosts < meta_hostfile
        echo "hosts=$hosts"
        StopServers "metadata" $hosts
    fi
    exit 0
fi

## To know if a configuration file exists in the default paths.
for FILE in {"/etc/hercules.conf","../conf/hercules.conf","./hercules.conf"}
do
    echo $FILE
    if [ -f "$FILE" ]; then
        echo "Reading configuration file at $FILE"
        break
    fi
done

## Check if user pass arguments.
while getopts :m:d:o:c: flag
do
    case "${flag}" in
        m) META_SERVER_FILE=${OPTARG};;
        d) DATA_SERVER_FILE=${OPTARG};;
        o) OUTPUT_FILE=${OPTARG};;
        c) CLIENT_FILE=${OPTARG};;
    esac
done

echo "DATA_SERVER_FILE=$DATA_SERVER_FILE"
echo "META_SERVER_FILE=$META_SERVER_FILE"
echo "OUTPUT_FILE=$OUTPUT_FILE"
echo "STATUS=$1"
## If slurm is used, then we create a hostfile containing the allocated nodes.
if [[ $SLURM -eq 1 ]]; then
    srun -pernode hostname |sort > hostfile
fi

## Define values.
H_PATH=$(dirname `pwd`)
H_BUILD_PATH=$H_PATH/build
H_BASH_PATH=$H_PATH/bash
H_MPI_HOSTFILE_NAME="client_hostfile"
H_POSIX_PRELOAD="LD_PRELOAD=$H_PATH/build/tools/libhercules_posix.so"

## Read configuration file.
META_PORT=$(cat $FILE | grep "METADATA_PORT" | awk '{print $3}')
DATA_PORT=$(cat $FILE | grep "DATA_PORT" | awk '{print $3}')
MALLEABILITY=$(cat $FILE | grep "MALLEABILITY" | awk '{print $3}')
NUM_METADATA=$(cat $FILE | grep "NUM_META_SERVERS" | awk '{print $3}')
NUM_DATA=$(cat $FILE | grep "NUM_DATA_SERVERS" | awk '{print $3}')
NUM_NODES_FOR_CLIENTS=$(cat $FILE | grep "NUM_NODES_FOR_CLIENTS" | awk '{print $3}')
NUM_CLIENTS_PER_NODE=$(cat $FILE | grep "NUM_CLIENTS_PER_NODE" | awk '{print $3}')
BLOCK_SIZE=$(cat $FILE | grep "BLOCK_SIZE" | awk '{print $3}')
STORAGE_SIZE=$(cat $FILE | grep "STORAGE_SIZE" | awk '{print $3}')

## To run the metadata servers.
echo "# Hercules: Running metadata servers"
## Checks if a metadata hostfile was not defined.
if [[ -z $META_SERVER_FILE ]];
then
    ## Error if slurm is not being used and no metadata hostfile was defined.
    if [[ $SLURM -eq 0 ]]; then
        echo "Metadata server file not specified, please set one using -m <filename> flag"
        exit 1
    fi
    echo "Metadata server file not specified, getting information from slurm"
    ## If a metadata file was not defined and we have slurm's nodes allocated,
    ## then we create an array which contains the hostnames of the nodes that
    ## will be used to deploy the determinate set of metadata servers.
    readarray -t meta_hosts < <(head -n $NUM_METADATA hostfile)
else
    ## If a metadata file was defined we read it to create an array which
    ## contains the hostnames of the nodes that will be used to deploy the
    ## determinate set of metadata servers.
    echo "Reading metadata server file."
    readarray -t meta_hosts < $META_SERVER_FILE
fi
## To create the default metadata hostfile.
# echo $meta_hosts > meta_hostfile
printf "%s\n" "${meta_hosts[@]}" > meta_hostfile

## The array is read to deploy the metadata servers.
i=0
start=`date +%s.%N`
for node in "${meta_hosts[@]}"
do
    echo "Running metadata server $i in $node..."
    ## If slurm is not being used, we deploy the service by connecting
    ## to the node via ssh.
    if [[ $SLURM -eq 0 ]]; then
        (ssh $node "cd $H_BASH_PATH && $H_BUILD_PATH/hercules_server m $i") &
    else
        ## If slurm is being used, the service is deploy using srun.
        srun --exclusive --export=ALL -N 1 -n 1 -w $node $H_BUILD_PATH/hercules_server m $i 2> m_server.out &
    fi
    i=$(($i+1))
done

## Wait until all metadata servers are up.
WaitForServers "metadata" "m" "${meta_hosts[@]}"
end=`date +%s.%N`
runtime=$( echo "$end - $start" | bc -l )
echo "Metadata servers started in $runtime seconds, start=$start, end=$end"

## To run the data servers.
echo "# Hercules: Running data servers"
if [[ -z $DATA_SERVER_FILE ]];
then
    if [[ $SLURM -eq 0 ]]; then
        echo "Data server file not specified, please set one using -d <filename> flag"
        exit 1
    fi
    echo "Data server file not specified, getting information from slurm"
    readarray -t data_hosts < <(tail -n +$((NUM_METADATA+1)) hostfile | head -n $NUM_DATA)
    echo "tail -n +$((NUM_METADATA+1)) hostfile | head -n $NUM_DATA"
else
    readarray -t data_hosts < $DATA_SERVER_FILE
fi

printf "%s\n" "${data_hosts[@]}" > data_hostfile
# META_NODE=${meta_hosts[0]}
# NUM_METADATA=${#meta_hosts[@]}
# NUM_DATA=${#data_hosts[@]}
i=0
start=`date +%s.%N`
for node in "${data_hosts[@]}"
do
    echo "Running data server $i in $node..."
    if [[ $SLURM -eq 0 ]]; then
        (ssh $node "cd $H_BASH_PATH && $H_BUILD_PATH/hercules_server d $i ${meta_hosts[0]}") &
    else
        srun --exclusive --export=ALL -N 1 -n 1 -w $node $H_BUILD_PATH/hercules_server d $i ${meta_hosts[0]} 2> d_server.out &
    fi
    i=$(($i+1))
done

# Wait until all data servers are up.
WaitForServers "data" "d" "${data_hosts[@]}"
end=`date +%s.%N`
runtime=$( echo "$end - $start" | bc -l )
echo "Data servers started in $runtime seconds, start=$start, end=$end"
#set -x
# readarray -t client_hosts < <(tail -n +$((NUM_METADATA+NUM_DATA+1)) hostfile | head -n $NUM_NODES_FOR_CLIENTS)
# echo $client_hosts
# rm client_hostfile
# for node in "${client_hosts[@]}"
# do
#     echo "Cheking $node information..."
#     # srun -N 1 -n 1 -w $node echo "Threads/core: $(nproc --all)" | awk '{print $2}' &
#     srun -N 1 -n $NUM_CLIENTS_PER_NODE -w $node hostname >> client_hostfile
# done

# tail -n +$((NUM_METADATA+NUM_DATA+1)) hostfile | head -n $NUM_NODES_FOR_CLIENTS | sed "s/$/\tslots=12/" > client_hostfile
tail -n +$((NUM_METADATA+NUM_DATA+1)) hostfile | head -n $NUM_NODES_FOR_CLIENTS > $H_MPI_HOSTFILE_NAME
export H_NCPN=$NUM_CLIENTS_PER_NODE

## Search for the mpi distribution installed.
for MPI_DS in {"openmpi","mpich","impi"}
do
    RET=$(which mpiexec | grep -c $MPI_DS)
    if [ $RET -gt 0 ]; then
        echo "MPI distribution found; $MPI_DS"
        break;
    fi
done

case $MPI_DS in
    "openmpi")
        echo "Option openmpi"
        export H_MPI_ENV_DEF="-x"
        export H_MPI_HOSTFILE_DEF="-hostfile"
    ;;
    "mpich" | "impi")
        echo "Option mpich | impi"
        export H_MPI_ENV_DEF="-env"
        export H_MPI_HOSTFILE_DEF="-f"
    ;;
    *)
        # Check!
        echo "No option"
        exit 1
    ;;
esac


unset META_PORT
unset DATA_PORT
unset MALLEABILITY
unset NUM_METADATA
unset NUM_DATA
unset NUM_NODES_FOR_CLIENTS
unset NUM_CLIENTS_PER_NODE
unset BLOCK_SIZE
unset STORAGE_SIZE

